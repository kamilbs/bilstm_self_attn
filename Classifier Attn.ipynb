{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import json\n",
    "from utils import preprocessing_helpers, dataset_helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_folder = 'data/data_sentiment140'\n",
    "train_path = os.path.join(data_folder, 'train_data.csv')\n",
    "eval_path = os.path.join(data_folder, 'eval_data.csv')\n",
    "test_path = os.path.join(data_folder, 'test_data.csv')\n",
    "vocab_file_path = os.path.join(data_folder, 'vocabulary_list.txt')\n",
    "label_vocab = ['negative', 'positive']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 10\n",
    "def train_input_fn():\n",
    "    dataset = dataset_helpers.make_dataset_from_csv(train_path, num_epochs, batch_size=128, shuffle=True, drop_remainder=True)\n",
    "    tokens, sequence_length, label = dataset.make_one_shot_iterator().get_next()\n",
    "    return {'tokens': tokens, 'sequence_length': sequence_length}, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_input_fn():\n",
    "    dataset = dataset_helpers.make_dataset_from_csv(eval_path, 1, batch_size=128, shuffle=False, drop_remainder=False)\n",
    "    tokens, sequence_length, label = dataset.make_one_shot_iterator().get_next()\n",
    "    return {'tokens': tokens, 'sequence_length': sequence_length}, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "fine_tune_embedding = True\n",
    "embedding_dimension = 150 # Should be the same as the pre-trained matrix\n",
    "\n",
    "def wrap_const(*args, **kwargs):\n",
    "    embedding_matrix = np.load(os.path.join(data_folder,'embedding_matrix.npy')).astype(np.float32)\n",
    "    return embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:vocabulary_size = 47874 in tokens is inferred from the number of elements in the vocabulary_file data/data_sentiment140/vocabulary_list.txt.\n"
     ]
    }
   ],
   "source": [
    "word_ids = tf.contrib.feature_column.sequence_categorical_column_with_vocabulary_file('tokens', vocab_file_path , num_oov_buckets=1)\n",
    "word_embeddings = tf.feature_column.embedding_column(word_ids, embedding_dimension, initializer=wrap_const,\n",
    "                                                     trainable=fine_tune_embedding)\n",
    "\n",
    "feature_columns = [tf.feature_column.numeric_column('sequence_length')]\n",
    "sequence_feature_columns = [word_embeddings]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Model HP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'num_classes': 2,\n",
    "    'num_hidden_units': 300,\n",
    "    'attention_dimension': 100,\n",
    "    'number_attention_hop': 30,\n",
    "    'num_dense_units': 500,\n",
    "    'penalty_coeff': 0,\n",
    "    'num_hidden_layers': 1,\n",
    "    'masking_attention': True, # Whether or not to mask and renormalize the attention weights (putting to 0 the one for PAD)\n",
    "    'feature_columns': feature_columns,\n",
    "    'sequence_feature_columns': sequence_feature_columns,\n",
    "    'label_vocab':label_vocab,\n",
    "    'visualize_attention': True, # Only for predict mode if we want to get the attention matrix in the predictions\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Model definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model.bilstm_self_attention import BiLSTMSelfAttention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_fn(features, labels, mode, params):\n",
    "    label_table = tf.contrib.lookup.index_table_from_tensor(mapping=tf.constant(params['label_vocab']), default_value=-1)\n",
    "    \n",
    "    sequence_length = tf.feature_column.input_layer(features, params['feature_columns'])\n",
    "    sequence_length = tf.cast(tf.reshape(sequence_length, [-1]),tf.int32)\n",
    "    \n",
    "    # Here we prefered to do the sequence length and padding direclty by the tf.data.Dataset for effeciency\n",
    "    # Therefore we won't take the sequence_length given by sequence_input_layer (it will be equal to the padded size for all the elements)\n",
    "    input_embeddings, _ = tf.contrib.feature_column.sequence_input_layer(features, params['sequence_feature_columns'])\n",
    "    \n",
    "    inputs = {'input_embeddings': input_embeddings, 'sequence_length':sequence_length}\n",
    "    # Model takes as input a dictionary with a 3D tensor (batch x max_seq_len x emb dimension) and a vector batch with the size of each example\n",
    "    model_bilstm_attn = BiLSTMSelfAttention(params)\n",
    "    logits, A = model_bilstm_attn(inputs)\n",
    "    \n",
    "    with tf.variable_scope('prediction'):\n",
    "        softmax = tf.nn.softmax(logits)\n",
    "        predictions = tf.argmax(logits, axis=1)\n",
    "        \n",
    "    if mode == tf.estimator.ModeKeys.PREDICT:\n",
    "        predictions_dict = {'probabilities': softmax,\n",
    "                            'predictions':predictions,\n",
    "                            'sequence_length':sequence_length}\n",
    "        if params['visualize_attention']:\n",
    "            predictions_dict['attention_matrix'] = A\n",
    "        return tf.estimator.EstimatorSpec(mode, predictions=predictions_dict)\n",
    "    \n",
    "    # ---- Training or Evaluation Mode ---\n",
    "    \n",
    "    # Convert string label to onehot encoding\n",
    "    labels = tf.squeeze(tf.one_hot(label_table.lookup(labels), len(params['label_vocab'])), axis=1)\n",
    "\n",
    "    with tf.variable_scope('metrics'):\n",
    "        accuracy = tf.metrics.accuracy(labels=tf.argmax(labels,axis=1), predictions=predictions, name='acc_op')\n",
    "\n",
    "    metrics = {'accuracy': accuracy}\n",
    "    tf.summary.scalar('accuracy', accuracy[1])\n",
    "\n",
    "    with tf.variable_scope('loss'):\n",
    "        P = tf.reduce_sum(tf.square(tf.matmul(A,tf.transpose(A,[0,2,1])) - tf.eye(params['number_attention_hop'])),[1,2]) \n",
    "        cross_entropy = tf.nn.softmax_cross_entropy_with_logits_v2(labels=labels, logits=logits)\n",
    "        loss = tf.reduce_mean(cross_entropy + params['penalty_coeff']*P)\n",
    "\n",
    "    with tf.variable_scope('summaries'):\n",
    "        with tf.variable_scope('loss'):\n",
    "            tf.summary.scalar('cross-entropy',tf.reduce_mean(cross_entropy))\n",
    "            tf.summary.scalar('penalty', tf.reduce_mean(P))\n",
    "            tf.summary.scalar('loss',loss)       \n",
    "\n",
    "    if mode == tf.estimator.ModeKeys.EVAL:\n",
    "        return tf.estimator.EstimatorSpec(mode, loss=loss, eval_metric_ops=metrics)\n",
    "    else:\n",
    "        with tf.variable_scope('optimizer'):\n",
    "            # Lazy Adam to handle sparse gradient updates (since we are not using all the words in each batch)\n",
    "            optimizer = tf.contrib.opt.LazyAdamOptimizer(learning_rate=1e-3)\n",
    "            train_op = optimizer.minimize(loss,global_step=tf.train.get_or_create_global_step())\n",
    "        return tf.estimator.EstimatorSpec(mode, loss=loss, train_op=train_op)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using config: {'_model_dir': './output_model3', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': 200, '_save_checkpoints_secs': None, '_session_config': allow_soft_placement: true\n",
      "graph_options {\n",
      "  rewrite_options {\n",
      "    meta_optimizer_iterations: ONE\n",
      "  }\n",
      "}\n",
      ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x11fa193c8>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n"
     ]
    }
   ],
   "source": [
    "run_config = tf.estimator.RunConfig(save_checkpoints_steps=200)\n",
    "estimator_model = tf.estimator.Estimator(model_fn,params=params,model_dir='./output_model3',config=run_config)\n",
    "train_spec = tf.estimator.TrainSpec(train_input_fn,max_steps=None)\n",
    "eval_spec = tf.estimator.EvalSpec(eval_input_fn, steps=None) # Evaluate over the whole evaluation dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tf.estimator.train_and_evaluate(estimator_model, train_spec, eval_spec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Prediction Part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_input_fn(data_gen):\n",
    "    def _predict_input_fn():\n",
    "        dataset = dataset_helpers.make_pred_dataset_from_gen(lambda: data_gen) #take callable to generator as input\n",
    "        tokens, sequence_length =  dataset.make_one_shot_iterator().get_next()\n",
    "        return {\"tokens\":tokens, \"sequence_length\":sequence_length}\n",
    "    return _predict_input_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "texts_to_predict = [\"I didn't enjoy it\", 'It was a wonderful experience']\n",
    "texts_to_predict_it = iter(texts_to_predict)\n",
    "res = estimator_model.predict(predict_input_fn(texts_to_predict_it), yield_single_examples=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from ./output_model3/model.ckpt-400\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n"
     ]
    }
   ],
   "source": [
    "val = next(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Attention Visualization part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.preprocessing_helpers import process_text_without_label\n",
    "from IPython.core.display import display, HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_index = 1\n",
    "processed_text, _ = process_text_without_label(texts_to_predict[example_index].encode('utf-8'))\n",
    "token_list = processed_text.decode('utf-8').split(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('negative', 0.00464689), ('positive', 0.9953531)]\n"
     ]
    }
   ],
   "source": [
    "print(list(zip(label_vocab, val['probabilities'][example_index])))\n",
    "att_matrix = val['attention_matrix'][example_index]\n",
    "content = list(zip(token_list, np.sum(att_matrix,axis=0)/np.sum(att_matrix).tolist()))\n",
    "html_content = ' '.join([f'<span style=\"background-color:rgba(255, 0, 0, {alpha});\">{token}</span>' for token,alpha in content])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span style=\"background-color:rgba(255, 0, 0, 0.14456988871097565);\">it</span> <span style=\"background-color:rgba(255, 0, 0, 0.1686643660068512);\">was</span> <span style=\"background-color:rgba(255, 0, 0, 0.269792377948761);\">a</span> <span style=\"background-color:rgba(255, 0, 0, 0.26377665996551514);\">wonderful</span> <span style=\"background-color:rgba(255, 0, 0, 0.15319670736789703);\">experience</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(HTML(html_content))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fresh_tf",
   "language": "python",
   "name": "fresh_tf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
